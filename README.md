# Software 10: Optical Music Recognition

## Training a model with the TF Object Detection API
Consider reading the official [Object Detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)
for a general guide to the API.
The guide below is specific to this project.
### Installation
Install the required packages from the Pipfile.

Install protobuf by [downloading](https://github.com/protocolbuffers/protobuf/releases)
the appropriate release for you OS. Extract the contents of the zip file, and 
add that directory to $PATH, by addding it to home/.bashrc.
Example: `export PATH=/home/username/protoc/bin:$PATH`.

Compile protobuf libraries (from within models/research):

```bash
protoc object_detection/protos/*.proto --python_out=.
```

More details about the last two steps can be found in the Object Detection API
[installation guide](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html).

Furthermore, cd into models/research and run:
```bash
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
```

### Data retrieval and preprocessing
To get the MUSCIMA++ dataset, run 

```bash
python src/data_handling/download_muscima.py
```

The Object Detection API requires the training data in TF Record files, and
these can be generated directly from xml files in Pascal VOC format.
Run the following command to generate Pascal VOC xml files from the annotations
(currently only supports full-page training):

```bash
python src/data_handling/prepare_muscima_full_page_annotations.py
```

Split the data into train, validation, and test:

```bash
python src/data_handling/dataset_splitter.py --source_directory=data/MUSCIMA++/full_page_annotations/annotations --destination_directory=data/MUSCIMA++/full_page_annotations/
```

The Object Detection API requires a mapping of class label names to id's, which can
be generated by running:

```bash
python src/data_handling/generate_mapping_for_muscima_pp.py
```

Finally, the TensorFlow record files for training, validation, and testing can
be generated from the Pascal VOC xml files and the label mapping:

```bash
python src/data_handling/generate_tfrecord.py -x data/full_page_annotations/training/ -l data/MUSCIMA++/mapping_all_classes.txt -o data/MUSCIMA++/full_page_annotations/train.record
python src/data_handling/generate_tfrecord.py -x data/full_page_annotations/validation/ -l data/MUSCIMA++/mapping_all_classes.txt -o data/MUSCIMA++/full_page_annotations/validation.record
python src/data_handling/generate_tfrecord.py -x data/full_page_annotations/test/ -l data/MUSCIMA++/mapping_all_classes.txt -o data/MUSCIMA++/full_page_annotations/test.record
```

### Training

Download a pre-trained model from the [TF2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)
and extract the model folder to `sw10/object_detection_api_training/pre_trained_models/`

The models with customised configs are:
- SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)
- Faster R-CNN Inception ResNet V2 1024x1024 (**WARNING**: running this model
  currently produces an error)
  
If you download a pre-trained model which is not listed above, follow
[this guide](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)
under the "Configure The Training Pipeline" section to set up the
`pipeline.config` file for the model.

To begin training, insert the correct folder names into the following command
and run it:

```bash
python src/run_model_w_obj_det_api.py --model_dir=object_detection_api_training/models/[MODEL FOLDER] --pipeline_config_path=object_detection_api_training/models/[MODEL FOLDER]/pipeline.config
```

If it fails because a module could not be found, run 
```export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim``` in models/research again.

### Evaluation

To evaluate a model, insert the correct folder names into the following 
command and run it:

```bash
python src/run_model_w_obj_det_api.py --model_dir=object_detection_api_training/models/[MODEL FOLDER] --pipeline_config_path=object_detection_api_training/models/[MODEL FOLDER]/pipeline.config --checkpoint_dir=object_detection_api_training/models/[MODEL FOLDER]
```

The model is evaluated using the specified metric in the pipeline.config file. 
The latest checkpoint of the model is used for evaluation, and the evaluation
process will check every 300 seconds for a new checkpoint to use.

The results are stored in TF event files at
`object_detection_api_training/models[MODEL FOLDER]/eval_0`.
These results can be monitored using TensorBoard.


### Exporting a trained model
To export a trained model, run the following command:

```bash
python 
```